{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10923340,"sourceType":"datasetVersion","datasetId":6791286},{"sourceId":10923348,"sourceType":"datasetVersion","datasetId":6791291},{"sourceId":10923354,"sourceType":"datasetVersion","datasetId":6791296},{"sourceId":11301114,"sourceType":"datasetVersion","datasetId":7067380},{"sourceId":11301391,"sourceType":"datasetVersion","datasetId":7067598},{"sourceId":11503208,"sourceType":"datasetVersion","datasetId":7212032}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:16:06.778079Z","iopub.execute_input":"2025-05-21T15:16:06.778379Z","iopub.status.idle":"2025-05-21T15:16:07.994831Z","shell.execute_reply.started":"2025-05-21T15:16:06.778346Z","shell.execute_reply":"2025-05-21T15:16:07.993859Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/acf-fd-table/FD_table.csv\n/kaggle/input/ped-data/data_with_ratios.csv\n/kaggle/input/acf-table-x/ACF_table.csv\n/kaggle/input/new-dataset/classification_FD.csv\n/kaggle/input/pcom-jcom/combined_output2.csv\n/kaggle/input/be-data/classification_FD_nonphysical.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Attribute selection**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:26:27.533819Z","iopub.execute_input":"2025-05-21T15:26:27.534202Z","iopub.status.idle":"2025-05-21T15:26:29.060959Z","shell.execute_reply.started":"2025-05-21T15:26:27.534172Z","shell.execute_reply":"2025-05-21T15:26:29.059658Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [20 30] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n","output_type":"stream"},{"name":"stdout","text":"Selected Feature Names: ['Jcom_pre5', 'Jcom_ratio5', 'Jcom_ratio9']\nCross-Validation Score: 0.5900\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef load_data(filepath):\n    \"\"\"Load data and filter columns starting with 'pcom' or 'Jcom'.\"\"\"\n    data = pd.read_csv(filepath)\n    # Keep only relevant columns\n    selected_columns = [col for col in data.columns]# if col.startswith((\"pcom\", \"Jcom\"))]\n    X = data[selected_columns]  # Select only matching columns\n    X = X.drop(['Sub_code','resp'], axis=1)\n    y = data['resp'].map({'performer': 1, 'nonperformer': 0}).fillna(0).astype(int)\n    return X, y\n\nclass WekaStyleSelector:\n    def __init__(self, k=3, seed=1, num_folds=10):\n        self.k = k\n        self.seed = seed\n        self.num_folds = num_folds\n        self.pipeline = None\n        self.feature_names = None\n\n    def fit(self, X, y):\n        \"\"\"Fit the pipeline (feature selection + classifier).\"\"\"\n        self.feature_names = X.columns.tolist()\n        selector = SelectKBest(score_func=mutual_info_classif, k=self.k)\n        clf = RandomForestClassifier(random_state=self.seed)\n        self.pipeline = Pipeline([('selector', selector), ('clf', clf)])\n        self.pipeline.fit(X, y)  # Fit on entire data (for inspection)\n\n    def transform(self, X):\n        \"\"\"Return features selected during fitting.\"\"\"\n        return self.pipeline.named_steps['selector'].transform(X)\n\n    def cross_validate(self, X, y):\n        \"\"\"Perform CV with per-fold feature selection (like Weka's FilteredClassifier).\"\"\"\n        cv = StratifiedKFold(n_splits=self.num_folds, shuffle=True, random_state=self.seed)\n        selector = SelectKBest(score_func=mutual_info_classif, k=self.k)\n        pipeline = Pipeline([('selector', selector), ('clf', RandomForestClassifier(random_state=self.seed))])\n        scores = cross_val_score(pipeline, X, y, cv=cv, n_jobs=-1)\n        return np.mean(scores)\n\n    def get_selected_feature_names(self):\n        \"\"\"Get names of features selected during fitting.\"\"\"\n        mask = self.pipeline.named_steps['selector'].get_support()\n        return [self.feature_names[i] for i in np.where(mask)[0]]\n\n# Load and shuffle data (Weka does not auto-shuffle; remove if undesired)\nX, y = load_data('/kaggle/input/ped-data/data_with_ratios.csv')\nX, y = shuffle(X, y, random_state=1)\n\n# Initialize and run selector\nselector = WekaStyleSelector(k=3, seed=1)\nselector.fit(X, y)\n\n\n# Get selected features (from full-dataset fit)\nselected_features = selector.get_selected_feature_names()\nprint(\"Selected Features (Weka-style):\", selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:04:01.958858Z","iopub.execute_input":"2025-05-21T17:04:01.959252Z","iopub.status.idle":"2025-05-21T17:04:02.217758Z","shell.execute_reply.started":"2025-05-21T17:04:01.959223Z","shell.execute_reply":"2025-05-21T17:04:02.216708Z"}},"outputs":[{"name":"stdout","text":"Selected Features (Weka-style): ['pcom_pre4', 'pcom_pre5', 'Jcom_pre5']\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"selected_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T16:48:03.869709Z","iopub.execute_input":"2025-05-21T16:48:03.870037Z","iopub.status.idle":"2025-05-21T16:48:03.875745Z","shell.execute_reply.started":"2025-05-21T16:48:03.870012Z","shell.execute_reply":"2025-05-21T16:48:03.874490Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"['kur_tonic_TS_lh2',\n 'skew_phasic_TS_rh5',\n 'nor_kur_phasic_TS_rh5',\n 'nor_var_phasic_TS_rh4',\n 'Jcom_rh2']"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"**Ranker**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import entropy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.utils.validation import check_X_y\n\nclass InfoGainAttributeEval(BaseEstimator):\n    \"\"\"Information Gain attribute evaluator\"\"\"\n    \n    def __init__(self, missing_merge=True, binarize=False):\n        self.missing_merge = missing_merge\n        self.binarize = binarize\n        self.info_gains_ = None\n        self.valid_features_ = []\n        self.feature_names_ = []\n        \n    def fit(self, X, y):\n        \"\"\"Build the information gain evaluator\"\"\"\n        # Add feature names capture\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_ = X.columns.tolist()\n        else:\n            self.feature_names_ = [f\"Feature_{i}\" for i in range(X.shape[1])]\n        X, y = check_X_y(X, y, dtype=None, force_all_finite='allow-nan')\n        self.valid_features_ = list(range(X.shape[1]))\n        self._preprocess_data(X, y)\n        return self\n        \n    def _preprocess_data(self, X, y):\n        \"\"\"Handle discretization/binarization and missing values\"\"\"\n        self.df = pd.DataFrame(X)\n        self.classes_ = pd.Series(y).unique()\n        \n        # Handle numeric features\n        for col in self.df.select_dtypes(include='number'):\n            if self.binarize:\n                self.df[col] = self._binarize(self.df[col])\n            else:\n                self.df[col] = self._discretize(self.df[col])\n                \n        # Store processed data and class labels\n        self.df['__class__'] = y\n        self._build_contingency_tables()\n        \n    def _discretize(self, feature):\n        \"\"\"Discretize numeric features using KBinsDiscretizer\"\"\"\n        discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n        return discretizer.fit_transform(feature.values.reshape(-1, 1)).ravel()\n    \n    def _binarize(self, feature):\n        \"\"\"Binarize numeric features using median threshold\"\"\"\n        return (feature > feature.median()).astype(int)\n    \n    def _build_contingency_tables(self):\n        \"\"\"Build contingency tables for each feature\"\"\"\n        self.contingency_tables = {}\n        \n        for col in self.df.columns[:-1]:  # Exclude class column\n            # Create contingency table\n            cont_table = pd.crosstab(\n                self.df[col], \n                self.df['__class__'],\n                rownames=[col],\n                colnames=['class'],\n                dropna=False\n            )\n            \n            # Handle missing values\n            if self.missing_merge:\n                cont_table = self._distribute_missing(cont_table)\n                \n            self.contingency_tables[col] = cont_table\n            \n    def _distribute_missing(self, cont_table):\n        \"\"\"Distribute missing values proportionally\"\"\"\n        # Calculate missing proportions\n        row_missing = cont_table.loc[np.nan] if np.nan in cont_table.index else pd.Series(0, index=cont_table.columns)\n        col_missing = cont_table.loc[:, np.nan] if np.nan in cont_table.columns else pd.Series(0, index=cont_table.index)\n        \n        # Remove missing entries\n        cont_table = cont_table.dropna(how='any', axis=0)\n        cont_table = cont_table.dropna(how='any', axis=1)\n        \n        # Calculate distribution proportions\n        row_totals = cont_table.sum(axis=1)\n        col_totals = cont_table.sum(axis=0)\n        total = cont_table.sum().sum()\n        \n        # Distribute row missing values\n        for idx, count in row_missing.items():\n            if count > 0 and total > 0:\n                proportions = row_totals / total\n                cont_table.loc[:, idx] += proportions * count\n                \n        # Distribute column missing values\n        for idx, count in col_missing.items():\n            if count > 0 and total > 0:\n                proportions = col_totals / total\n                cont_table.loc[idx, :] += proportions * count\n                \n        return cont_table.fillna(0)\n    \n    def _calculate_entropy(self, cont_table):\n        \"\"\"Calculate entropy for a contingency table\"\"\"\n        class_counts = cont_table.sum(axis=0)\n        total = class_counts.sum()\n        class_probs = class_counts / total\n        return entropy(class_probs, base=2)\n    \n    def _calculate_conditional_entropy(self, cont_table):\n        \"\"\"Calculate conditional entropy for a feature\"\"\"\n        feature_counts = cont_table.sum(axis=1)\n        total = feature_counts.sum()\n        entropies = []\n        \n        for _, row in cont_table.iterrows():\n            row_total = row.sum()\n            if row_total == 0:\n                continue\n            probs = row / row_total\n            ent = entropy(probs, base=2)\n            entropies.append((row_total / total) * ent)\n            \n        return sum(entropies)\n    \n    def evaluate_attribute(self, attribute_idx):\n        \"\"\"Evaluate information gain for a specific attribute\"\"\"\n        if self.info_gains_ is None:\n            self._compute_info_gains()\n            \n        if attribute_idx >= len(self.info_gains_):\n            raise ValueError(f\"Invalid attribute index: {attribute_idx}\")\n            \n        return self.info_gains_[attribute_idx]\n    \n    def _compute_info_gains(self):\n        \"\"\"Compute information gains for all attributes\"\"\"\n        self.info_gains_ = []\n        \n        # Calculate overall entropy\n        class_counts = self.df['__class__'].value_counts()\n        total = class_counts.sum()\n        class_probs = class_counts / total\n        H_class = entropy(class_probs, base=2)\n        \n        for col in self.df.columns[:-1]:\n            cont_table = self.contingency_tables[col]\n            H_conditional = self._calculate_conditional_entropy(cont_table)\n            self.info_gains_.append(H_class - H_conditional)\n    \n    def get_ranked_features(self):\n        \"\"\"Return features ranked by information gain\"\"\"\n        if self.info_gains_ is None:\n            self._compute_info_gains()\n            \n        # Create list of (index, gain) pairs\n        features = list(range(len(self.info_gains_)))\n        ranked = sorted(zip(features, self.info_gains_), \n                        key=lambda x: x[1], reverse=True)\n        return ranked\n\n    def print_ranking(self):\n        \"\"\"Print feature ranking with information gain scores\"\"\"\n        ranked = self.get_ranked_features()\n        print(\"\\nFeature Ranking based on Information Gain:\")\n        print(\"Rank\\tFeature\\t\\tInformation Gain\")\n        print(\"-------------------------------------------\")\n        for i, (feat_idx, gain) in enumerate(ranked):\n            # Use actual feature names\n            feat_name = self.feature_names_[feat_idx]\n            print(f\"{i+1}\\t{feat_name}\\t\\t{gain:.4f}\")\n\n\n    \n    \n# Initialize and fit evaluator\nig = InfoGainAttributeEval()\nX=X[selected_features]\nig.fit(X, y)\n    \nranked = ig.print_ranking()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:04:12.403870Z","iopub.execute_input":"2025-05-21T17:04:12.404282Z","iopub.status.idle":"2025-05-21T17:04:12.478551Z","shell.execute_reply.started":"2025-05-21T17:04:12.404252Z","shell.execute_reply":"2025-05-21T17:04:12.477673Z"}},"outputs":[{"name":"stdout","text":"\nFeature Ranking based on Information Gain:\nRank\tFeature\t\tInformation Gain\n-------------------------------------------\n1\tpcom_pre5\t\t0.1575\n2\tpcom_pre4\t\t0.1430\n3\tJcom_pre5\t\t0.0727\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n                             cohen_kappa_score, roc_auc_score, precision_recall_curve, auc,\n                             matthews_corrcoef)\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nclass WekaStyleRandomForest:\n    def __init__(self, num_trees=100, max_depth=None, seed=1):\n        \"\"\"\n        Mirrors Weka's RandomForest defaults:\n        - numTrees = 100\n        - maxDepth = unlimited (None)\n        - use entropy for splitting (criterion='entropy')\n        - compute out-of-bag (OOB) error (oob_score=True)\n        - feature selection: log2(num_features) + 1 (max_features='log2')\n        \"\"\"\n        self.classifier = RandomForestClassifier(\n            n_estimators=num_trees,\n            criterion='entropy',  # Weka uses entropy, not Gini\n            max_depth=max_depth,\n            max_features='log2',   # Weka's default feature selection\n            oob_score=True,        # Enable OOB error (like Weka)\n            random_state=seed,\n            n_jobs=-1\n        )\n\n    def cross_validate(self, X, y, num_folds=10):\n        \"\"\"Return both predicted labels and probabilities.\"\"\"\n        skf = StratifiedKFold(n_splits=num_folds, shuffle=False)\n        y_proba = cross_val_predict(\n            self.classifier, X, y, cv=skf, method='predict_proba'\n        )[:, 1]  # Probabilities of class 1\n        y_pred = np.round(y_proba).astype(int)  # Convert to class labels\n        return y_pred, y_proba  # Return both\n\n    def get_weka_metrics(self, y_true, y_pred, y_proba):\n        \"\"\"Use probabilities to compute ROC/PRC AUC.\"\"\"\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n        report = classification_report(y_true, y_pred, target_names=['Performer', 'NonPerpormer'], output_dict=True)\n        \n        # Compute AUC using probabilities\n        roc_auc = roc_auc_score(y_true, y_proba)\n        precision, recall, _ = precision_recall_curve(y_true, y_proba)\n        prc_auc = auc(recall, precision)\n        \n # Regression-style metrics\n        mae = mean_absolute_error(y_true, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n        relative_absolute_error = mae / np.mean(np.abs(y_true))\n        root_relative_squared_error = rmse / np.sqrt(np.mean(y_true**2))\n        \n        return {\n            \"confusion_matrix\": [[tn, fp], [fn, tp]],\n            \"accuracy\": accuracy_score(y_true, y_pred),\n            \"kappa\": cohen_kappa_score(y_true, y_pred),\n            \"roc_auc\": roc_auc,\n            \"prc_auc\": prc_auc,\n            \"mcc\": matthews_corrcoef(y_true, y_pred),\n            \"precision\": report['Performer']['precision'],\n            \"recall\": report['Performer']['recall'],\n            \"f1\": report['Performer']['f1-score'],\n            \"weighted_avg_f1\": report['weighted avg']['f1-score'],\n            \"mae\": mae,\n            \"rmse\": rmse,\n            \"relative_absolute_error\": relative_absolute_error,\n            \"root_relative_squared_error\": root_relative_squared_error\n        }\n\n\n\n# Initialize Weka-style classifier\nrf = WekaStyleRandomForest(num_trees=100, max_depth=None, seed=1)\n\n# Cross-validate (no shuffling, 10 folds)\ny_pred, y_proba = rf.cross_validate(X, y, num_folds=10)\n\n\n# Get metrics\nmetrics = rf.get_weka_metrics(y, y_pred, y_proba)  # Add y_proba if using predict_proba\n\n\n# Print results (Weka-like format)\nprint(f\"Correctly Classified Instances: {metrics['accuracy'] * len(y)} ({metrics['accuracy'] * 100:.2f}%)\")\nprint(f\"Incorrectly Classified Instances: {(1 - metrics['accuracy']) * len(y)} ({(1 - metrics['accuracy']) * 100:.2f}%)\")\nprint(f\"Kappa Statistic: {metrics['kappa']:.4f}\")\nprint(f\"Matthews Correlation Coefficient: {metrics['mcc']:.4f}\")\nprint(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\nprint(f\"PRC AUC: {metrics['prc_auc']:.4f}\")\nprint(f\"Precision (Performer): {metrics['precision']:.4f}\")\nprint(f\"Recall (Performer): {metrics['recall']:.4f}\")\nprint(f\"F1-Score (Performer): {metrics['f1']:.4f}\")\nprint(f\"Weighted Avg F1-Score: {metrics['weighted_avg_f1']:.4f}\")\nprint(f\"Mean Absolute Error: {metrics['mae']:.4f}\")\nprint(f\"Root Mean Squared Error: {metrics['rmse']:.4f}\")\nprint(f\"Relative Absolute Error: {metrics['relative_absolute_error']:.4f}\")\nprint(f\"Root Relative Squared Error: {metrics['root_relative_squared_error']:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(pd.DataFrame(metrics['confusion_matrix'], \n                   index=['Nonperformer (Actual)', 'Performer (Actual)'], \n                   columns=['Nonperformer (Predicted)', 'Performer (Predicted)']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:04:19.281959Z","iopub.execute_input":"2025-05-21T17:04:19.282289Z","iopub.status.idle":"2025-05-21T17:04:22.176260Z","shell.execute_reply.started":"2025-05-21T17:04:19.282262Z","shell.execute_reply":"2025-05-21T17:04:22.175405Z"}},"outputs":[{"name":"stdout","text":"Correctly Classified Instances: 77.0 (77.00%)\nIncorrectly Classified Instances: 23.0 (23.00%)\nKappa Statistic: 0.5344\nMatthews Correlation Coefficient: 0.5345\nROC AUC: 0.7841\nPRC AUC: 0.7775\nPrecision (Performer): 0.7333\nRecall (Performer): 0.7500\nF1-Score (Performer): 0.7416\nWeighted Avg F1-Score: 0.7703\nMean Absolute Error: 0.2300\nRoot Mean Squared Error: 0.4796\nRelative Absolute Error: 0.4107\nRoot Relative Squared Error: 0.6409\n\nConfusion Matrix:\n                       Nonperformer (Predicted)  Performer (Predicted)\nNonperformer (Actual)                        33                     11\nPerformer (Actual)                           12                     44\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"**Compute Ratios**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef calculate_ratios(input_file, output_file):\n    try:\n        # Read the stacked CSV file\n        df = pd.read_csv(input_file)\n        \n        # Add ratio columns for pcom values\n        # pcom_ratio1 = pcom_pre1/pcom_pre1\n        df['pcom_ratio1'] = df['pcom_pre1'] / df['pcom_pre1']  # This will be 1.0 for all non-NaN values\n        \n        # pcom_ratio2 through pcom_ratio5\n        for i in range(2, 6):\n            df[f'pcom_ratio{i}'] = df[f'pcom_pre{i}'] / df['pcom_pre1']\n        \n        # pcom_ratio6 through pcom_ratio10\n        for i in range(1, 6):\n            ratio_index = i + 5\n            df[f'pcom_ratio{ratio_index}'] = df[f'pcom_post{i}'] / df['pcom_pre1']\n        \n        # Add ratio columns for Jcom values - same pattern as pcom\n        # Jcom_ratio1 = Jcom_pre1/Jcom_pre1\n        df['Jcom_ratio1'] = df['Jcom_pre1'] / df['Jcom_pre1']  # This will be 1.0 for all non-NaN values\n        \n        # Jcom_ratio2 through Jcom_ratio5\n        for i in range(2, 6):\n            df[f'Jcom_ratio{i}'] = df[f'Jcom_pre{i}'] / df['Jcom_pre1']\n        \n        # Jcom_ratio6 through Jcom_ratio10\n        for i in range(1, 6):\n            ratio_index = i + 5\n            df[f'Jcom_ratio{ratio_index}'] = df[f'Jcom_post{i}'] / df['Jcom_pre1']\n        \n        # Save to a new CSV file\n        df.to_csv(output_file, index=False)\n        print(f\"Successfully added ratio columns and saved to {output_file}\")\n        return True\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n\ninput_file = \"stacked_hemispheres_data.csv\"  # Output from previous script\noutput_file = \"data_with_ratios.csv\"  \ncalculate_ratios(input_file, output_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:28:11.739164Z","iopub.execute_input":"2025-04-06T19:28:11.739553Z","iopub.status.idle":"2025-04-06T19:28:11.773199Z","shell.execute_reply.started":"2025-04-06T19:28:11.739519Z","shell.execute_reply":"2025-04-06T19:28:11.772099Z"}},"outputs":[],"execution_count":null}]}
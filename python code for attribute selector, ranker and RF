{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10923340,"sourceType":"datasetVersion","datasetId":6791286},{"sourceId":10923348,"sourceType":"datasetVersion","datasetId":6791291},{"sourceId":10923354,"sourceType":"datasetVersion","datasetId":6791296}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:00:07.200663Z","iopub.execute_input":"2025-04-06T19:00:07.200913Z","iopub.status.idle":"2025-04-06T19:00:08.109264Z","shell.execute_reply.started":"2025-04-06T19:00:07.200888Z","shell.execute_reply":"2025-04-06T19:00:08.107614Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/acf-table-x/ACF_table.csv\n/kaggle/input/acf-fd-table/FD_table.csv\n/kaggle/input/pcom-jcom/combined_output2.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Attribute selection**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef load_data(filepath):\n    \"\"\"\n    Load dataset from a file and keep only columns that start with 'pcom', 'Jcom', or 'J2max'.\n    \n    :param filepath: Path to the dataset file (CSV).\n    :return: Tuple (X, y) where X are the filtered features and y is the target.\n    \"\"\"\n    data = pd.read_csv(filepath)\n    \n    # Keep only relevant columns\n    selected_columns = [col for col in data.columns if col.startswith((\"pcom\", \"Jcom\"))]\n    X = data[selected_columns]  # Select only matching columns\n    y = data.iloc[:, -1]        # Last column as target\n    \n    return X, y\n\nclass AttributeSelection:\n    def __init__(self, k=10, method=\"f_classif\", seed=1, num_folds=10):\n        \"\"\"\n        Initializes the attribute selection model.\n        \n        :param k: Number of top features to select.\n        :param method: Scoring method ('f_classif' or 'mutual_info').\n        :param seed: Random seed for reproducibility.\n        :param num_folds: Number of folds for cross-validation.\n        \"\"\"\n        self.k = k\n        self.method = method\n        self.seed = seed\n        self.num_folds = num_folds\n        self.selected_features = None\n        self.selector = None\n        self.feature_names = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Performs feature selection on the dataset.\n        \n        :param X: Feature matrix (numpy array or pandas DataFrame).\n        :param y: Target labels.\n        \"\"\"\n        if self.method == \"f_classif\":\n            score_func = f_classif\n        elif self.method == \"mutual_info\":\n            score_func = mutual_info_classif\n        else:\n            raise ValueError(\"Invalid method. Choose 'f_classif' or 'mutual_info'.\")\n\n        self.selector = SelectKBest(score_func=score_func, k=self.k)\n        self.selector.fit(X, y)\n        self.selected_features = self.selector.get_support(indices=True)\n        self.feature_names = X.columns  # Store feature names\n\n    def transform(self, X):\n        \"\"\"\n        Transforms the dataset by selecting important features.\n        \n        :param X: Feature matrix.\n        :return: Transformed dataset with selected features.\n        \"\"\"\n        if self.selector is None:\n            raise ValueError(\"Feature selection has not been performed yet!\")\n        return self.selector.transform(X)\n\n    def cross_validate(self, X, y, model):\n        \"\"\"\n        Performs cross-validation to evaluate feature selection effectiveness.\n        \n        :param X: Feature matrix.\n        :param y: Target labels.\n        :param model: Machine learning model for evaluation.\n        :return: Mean accuracy score from cross-validation.\n        \"\"\"\n        X_selected = self.transform(X)\n        scores = cross_val_score(model, X_selected, y, cv=self.num_folds)\n        return np.mean(scores)\n\n    def get_selected_features(self):\n        \"\"\"\n        Returns the selected feature indices.\n        \n        :return: List of selected feature indices.\n        \"\"\"\n        if self.selected_features is None:\n            raise ValueError(\"Feature selection has not been performed yet!\")\n        return self.selected_features\n\n    def get_selected_feature_names(self):\n        \"\"\"\n        Get the names of selected features.\n        \n        :return: List of selected feature names.\n        \"\"\"\n        if self.selector is None or self.feature_names is None:\n            raise ValueError(\"Feature selection has not been performed yet!\")\n        return self.feature_names[self.selected_features].tolist()\n\n\n# Load dataset and filter only required columns\nX, y = load_data('/kaggle/input/ped-data/data_with_ratios.csv')\n\n# Shuffle dataset\nX, y = shuffle(X, y, random_state=1)\n\n# Perform attribute selection\nselector = AttributeSelection(k=5, method=\"f_classif\")\nselector.fit(X, y)\n\n# Transform dataset\nX_selected = selector.transform(X)\n\n# Print selected feature names\nselected_features=selector.get_selected_feature_names()\nprint(\"Selected Feature Names:\", selected_features)\n\n# Evaluate with a classifier\nmodel = RandomForestClassifier(random_state=1)\nscore = selector.cross_validate(X, y, model)\nprint(f\"Cross-Validation Score: {score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:39:53.925594Z","iopub.execute_input":"2025-04-06T19:39:53.926733Z","iopub.status.idle":"2025-04-06T19:39:55.566639Z","shell.execute_reply.started":"2025-04-06T19:39:53.926588Z","shell.execute_reply":"2025-04-06T19:39:55.565373Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [20 30] are constant.\n  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n  f = msb / msw\n","output_type":"stream"},{"name":"stdout","text":"Selected Feature Names: ['Jcom_post4', 'Jcom_pre5', 'pcom_ratio9', 'Jcom_ratio5', 'Jcom_ratio9']\nCross-Validation Score: 0.6800\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"selected_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:40:03.816542Z","iopub.execute_input":"2025-04-06T19:40:03.817034Z","iopub.status.idle":"2025-04-06T19:40:03.824458Z","shell.execute_reply.started":"2025-04-06T19:40:03.816968Z","shell.execute_reply":"2025-04-06T19:40:03.823320Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['Jcom_post4', 'Jcom_pre5', 'pcom_ratio9', 'Jcom_ratio5', 'Jcom_ratio9']"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"**Ranker**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import entropy\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.utils.validation import check_X_y\n\nclass InfoGainAttributeEval(BaseEstimator):\n    \"\"\"Information Gain attribute evaluator\"\"\"\n    \n    def __init__(self, missing_merge=True, binarize=False):\n        self.missing_merge = missing_merge\n        self.binarize = binarize\n        self.info_gains_ = None\n        self.valid_features_ = []\n        self.feature_names_ = []\n        \n    def fit(self, X, y):\n        \"\"\"Build the information gain evaluator\"\"\"\n        # Add feature names capture\n        if isinstance(X, pd.DataFrame):\n            self.feature_names_ = X.columns.tolist()\n        else:\n            self.feature_names_ = [f\"Feature_{i}\" for i in range(X.shape[1])]\n        X, y = check_X_y(X, y, dtype=None, force_all_finite='allow-nan')\n        self.valid_features_ = list(range(X.shape[1]))\n        self._preprocess_data(X, y)\n        return self\n        \n    def _preprocess_data(self, X, y):\n        \"\"\"Handle discretization/binarization and missing values\"\"\"\n        self.df = pd.DataFrame(X)\n        self.classes_ = pd.Series(y).unique()\n        \n        # Handle numeric features\n        for col in self.df.select_dtypes(include='number'):\n            if self.binarize:\n                self.df[col] = self._binarize(self.df[col])\n            else:\n                self.df[col] = self._discretize(self.df[col])\n                \n        # Store processed data and class labels\n        self.df['__class__'] = y\n        self._build_contingency_tables()\n        \n    def _discretize(self, feature):\n        \"\"\"Discretize numeric features using KBinsDiscretizer\"\"\"\n        discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n        return discretizer.fit_transform(feature.values.reshape(-1, 1)).ravel()\n    \n    def _binarize(self, feature):\n        \"\"\"Binarize numeric features using median threshold\"\"\"\n        return (feature > feature.median()).astype(int)\n    \n    def _build_contingency_tables(self):\n        \"\"\"Build contingency tables for each feature\"\"\"\n        self.contingency_tables = {}\n        \n        for col in self.df.columns[:-1]:  # Exclude class column\n            # Create contingency table\n            cont_table = pd.crosstab(\n                self.df[col], \n                self.df['__class__'],\n                rownames=[col],\n                colnames=['class'],\n                dropna=False\n            )\n            \n            # Handle missing values\n            if self.missing_merge:\n                cont_table = self._distribute_missing(cont_table)\n                \n            self.contingency_tables[col] = cont_table\n            \n    def _distribute_missing(self, cont_table):\n        \"\"\"Distribute missing values proportionally\"\"\"\n        # Calculate missing proportions\n        row_missing = cont_table.loc[np.nan] if np.nan in cont_table.index else pd.Series(0, index=cont_table.columns)\n        col_missing = cont_table.loc[:, np.nan] if np.nan in cont_table.columns else pd.Series(0, index=cont_table.index)\n        \n        # Remove missing entries\n        cont_table = cont_table.dropna(how='any', axis=0)\n        cont_table = cont_table.dropna(how='any', axis=1)\n        \n        # Calculate distribution proportions\n        row_totals = cont_table.sum(axis=1)\n        col_totals = cont_table.sum(axis=0)\n        total = cont_table.sum().sum()\n        \n        # Distribute row missing values\n        for idx, count in row_missing.items():\n            if count > 0 and total > 0:\n                proportions = row_totals / total\n                cont_table.loc[:, idx] += proportions * count\n                \n        # Distribute column missing values\n        for idx, count in col_missing.items():\n            if count > 0 and total > 0:\n                proportions = col_totals / total\n                cont_table.loc[idx, :] += proportions * count\n                \n        return cont_table.fillna(0)\n    \n    def _calculate_entropy(self, cont_table):\n        \"\"\"Calculate entropy for a contingency table\"\"\"\n        class_counts = cont_table.sum(axis=0)\n        total = class_counts.sum()\n        class_probs = class_counts / total\n        return entropy(class_probs, base=2)\n    \n    def _calculate_conditional_entropy(self, cont_table):\n        \"\"\"Calculate conditional entropy for a feature\"\"\"\n        feature_counts = cont_table.sum(axis=1)\n        total = feature_counts.sum()\n        entropies = []\n        \n        for _, row in cont_table.iterrows():\n            row_total = row.sum()\n            if row_total == 0:\n                continue\n            probs = row / row_total\n            ent = entropy(probs, base=2)\n            entropies.append((row_total / total) * ent)\n            \n        return sum(entropies)\n    \n    def evaluate_attribute(self, attribute_idx):\n        \"\"\"Evaluate information gain for a specific attribute\"\"\"\n        if self.info_gains_ is None:\n            self._compute_info_gains()\n            \n        if attribute_idx >= len(self.info_gains_):\n            raise ValueError(f\"Invalid attribute index: {attribute_idx}\")\n            \n        return self.info_gains_[attribute_idx]\n    \n    def _compute_info_gains(self):\n        \"\"\"Compute information gains for all attributes\"\"\"\n        self.info_gains_ = []\n        \n        # Calculate overall entropy\n        class_counts = self.df['__class__'].value_counts()\n        total = class_counts.sum()\n        class_probs = class_counts / total\n        H_class = entropy(class_probs, base=2)\n        \n        for col in self.df.columns[:-1]:\n            cont_table = self.contingency_tables[col]\n            H_conditional = self._calculate_conditional_entropy(cont_table)\n            self.info_gains_.append(H_class - H_conditional)\n    \n    def get_ranked_features(self):\n        \"\"\"Return features ranked by information gain\"\"\"\n        if self.info_gains_ is None:\n            self._compute_info_gains()\n            \n        # Create list of (index, gain) pairs\n        features = list(range(len(self.info_gains_)))\n        ranked = sorted(zip(features, self.info_gains_), \n                        key=lambda x: x[1], reverse=True)\n        return ranked\n\n    def print_ranking(self):\n        \"\"\"Print feature ranking with information gain scores\"\"\"\n        ranked = self.get_ranked_features()\n        print(\"\\nFeature Ranking based on Information Gain:\")\n        print(\"Rank\\tFeature\\t\\tInformation Gain\")\n        print(\"-------------------------------------------\")\n        for i, (feat_idx, gain) in enumerate(ranked):\n            # Use actual feature names\n            feat_name = self.feature_names_[feat_idx]\n            print(f\"{i+1}\\t{feat_name}\\t\\t{gain:.4f}\")\n\n\n    \n    \n# Initialize and fit evaluator\nig = InfoGainAttributeEval()\nX=X[selected_features]\nig.fit(X, y)\n    \nranked = ig.print_ranking()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:40:12.133598Z","iopub.execute_input":"2025-04-06T19:40:12.134020Z","iopub.status.idle":"2025-04-06T19:40:12.285158Z","shell.execute_reply.started":"2025-04-06T19:40:12.133964Z","shell.execute_reply":"2025-04-06T19:40:12.284084Z"}},"outputs":[{"name":"stdout","text":"\nFeature Ranking based on Information Gain:\nRank\tFeature\t\tInformation Gain\n-------------------------------------------\n1\tpcom_ratio9\t\t0.1612\n2\tJcom_ratio9\t\t0.1453\n3\tJcom_ratio5\t\t0.0778\n4\tJcom_pre5\t\t0.0727\n5\tJcom_post4\t\t0.0572\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"#remove the first 10 features\nnon_pcom_features = [feature for feature in selected_features \n                    if not str(feature).lower().startswith(\"pcom\")]\nprint(non_pcom_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:31:35.420166Z","iopub.execute_input":"2025-03-12T07:31:35.420585Z","iopub.status.idle":"2025-03-12T07:31:35.426022Z","shell.execute_reply.started":"2025-03-12T07:31:35.420550Z","shell.execute_reply":"2025-03-12T07:31:35.424877Z"}},"outputs":[{"name":"stdout","text":"['jcom_pre2', 'jcom_pre4', 'jcom_pre5', 'jcom_post2', 'jcom_post3', 'J2max_pre2', 'J2max_pre5', 'J2max_post2', 'J2max_post4', 'J2max_post5']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, mean_absolute_error,\n                             mean_squared_error, cohen_kappa_score, roc_auc_score, precision_recall_curve, matthews_corrcoef)\n\nclass RandomForestKaggle:\n    def __init__(self, num_trees=100, max_depth=5, random_seed=42, num_execution_slots=-1):\n        self.num_trees = num_trees\n        self.max_depth = max_depth\n        self.random_seed = random_seed\n        self.num_execution_slots = num_execution_slots\n        self.classifier = RandomForestClassifier(\n            n_estimators=self.num_trees,\n            max_depth=self.max_depth,\n            random_state=self.random_seed,\n            n_jobs=self.num_execution_slots\n        )\n\n    def cross_validate(self, X, y, num_folds=5):\n        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=self.random_seed)\n        y_pred = cross_val_predict(self.classifier, X, y, cv=skf)\n        return y_pred\n\n    def get_feature_importance(self, feature_names):\n        return dict(zip(feature_names, self.classifier.feature_importances_))\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/ped-data/data_with_ratios.csv\")\n\n\n#selected_features = ['pcom_post4', 'pcom_post5', 'pcom_post3', 'pcom_post1', 'pcom_pre3', 'pcom_pre2', 'pcom_pre4', 'pcom_pre5', 'pcom_pre1', 'pcom_post2', 'J2max_pre5', 'J2max_post5']\nclass_column = 'resp'\n\ndf[class_column] = df[class_column].map({'performer': 1, 'nonperformer': 0}).fillna(0).astype(int)\nX = df[selected_features].values\ny = df[class_column].values\n\n# Initialize classifier\nrf = RandomForestKaggle()\nnum_folds = 52\n\n# Perform cross-validation\ny_pred = rf.cross_validate(X, y, num_folds)\naccuracy = accuracy_score(y, y_pred)\nprint(f\"Cross-Validation Accuracy: {accuracy * 100:.2f}%\")\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y, y_pred, labels=[0, 1])\nconf_matrix_df = pd.DataFrame(conf_matrix, index=['Nonperformer', 'Performer'], columns=['Nonperformer', 'Performer'])\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_df)\n#print(\"Confusion Matrix:\")\n#print(conf_matrix)\n\n# Classification report\nreport = classification_report(y, y_pred, target_names=['Nonperformer', 'Performer'])\nprint(\"Classification Report:\")\nprint(report)\n\n# Additional metrics\nkappa = cohen_kappa_score(y, y_pred)\nmae = mean_absolute_error(y, y_pred)\nrmse = mean_squared_error(y, y_pred, squared=False)\nrelative_absolute_error = mae / np.mean(np.abs(y))\nroot_relative_squared_error = rmse / np.sqrt(np.mean(y**2))\n\nprint(f\"Correctly Classified Instances: {accuracy * len(y)} ({accuracy * 100:.2f}%)\")\nprint(f\"Incorrectly Classified Instances: {(1 - accuracy) * len(y)} ({(1 - accuracy) * 100:.2f}%)\")\nprint(f\"Kappa Statistic: {kappa:.4f}\")\nprint(f\"Mean Absolute Error: {mae:.4f}\")\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\nprint(f\"Relative Absolute Error: {relative_absolute_error:.4f}\")\nprint(f\"Root Relative Squared Error: {root_relative_squared_error:.4f}\")\nprint(f\"Total Number of Instances: {len(y)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:45:22.578897Z","iopub.execute_input":"2025-04-06T19:45:22.579302Z","iopub.status.idle":"2025-04-06T19:45:38.200135Z","shell.execute_reply.started":"2025-04-06T19:45:22.579268Z","shell.execute_reply":"2025-04-06T19:45:38.198921Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 44 members, which is less than n_splits=52.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Cross-Validation Accuracy: 68.00%\nConfusion Matrix:\n              Nonperformer  Performer\nNonperformer            23         21\nPerformer               11         45\nClassification Report:\n              precision    recall  f1-score   support\n\nNonperformer       0.68      0.52      0.59        44\n   Performer       0.68      0.80      0.74        56\n\n    accuracy                           0.68       100\n   macro avg       0.68      0.66      0.66       100\nweighted avg       0.68      0.68      0.67       100\n\nCorrectly Classified Instances: 68.0 (68.00%)\nIncorrectly Classified Instances: 31.999999999999996 (32.00%)\nKappa Statistic: 0.3344\nMean Absolute Error: 0.3200\nRoot Mean Squared Error: 0.5657\nRelative Absolute Error: 0.5714\nRoot Relative Squared Error: 0.7559\nTotal Number of Instances: 100\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef calculate_ratios(input_file, output_file):\n    try:\n        # Read the stacked CSV file\n        df = pd.read_csv(input_file)\n        \n        # Add ratio columns for pcom values\n        # pcom_ratio1 = pcom_pre1/pcom_pre1\n        df['pcom_ratio1'] = df['pcom_pre1'] / df['pcom_pre1']  # This will be 1.0 for all non-NaN values\n        \n        # pcom_ratio2 through pcom_ratio5\n        for i in range(2, 6):\n            df[f'pcom_ratio{i}'] = df[f'pcom_pre{i}'] / df['pcom_pre1']\n        \n        # pcom_ratio6 through pcom_ratio10\n        for i in range(1, 6):\n            ratio_index = i + 5\n            df[f'pcom_ratio{ratio_index}'] = df[f'pcom_post{i}'] / df['pcom_pre1']\n        \n        # Add ratio columns for Jcom values - same pattern as pcom\n        # Jcom_ratio1 = Jcom_pre1/Jcom_pre1\n        df['Jcom_ratio1'] = df['Jcom_pre1'] / df['Jcom_pre1']  # This will be 1.0 for all non-NaN values\n        \n        # Jcom_ratio2 through Jcom_ratio5\n        for i in range(2, 6):\n            df[f'Jcom_ratio{i}'] = df[f'Jcom_pre{i}'] / df['Jcom_pre1']\n        \n        # Jcom_ratio6 through Jcom_ratio10\n        for i in range(1, 6):\n            ratio_index = i + 5\n            df[f'Jcom_ratio{ratio_index}'] = df[f'Jcom_post{i}'] / df['Jcom_pre1']\n        \n        # Save to a new CSV file\n        df.to_csv(output_file, index=False)\n        print(f\"Successfully added ratio columns and saved to {output_file}\")\n        return True\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n\n\ninput_file = \"stacked_hemispheres_data.csv\"  # Output from previous script\noutput_file = \"data_with_ratios.csv\"  \ncalculate_ratios(input_file, output_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:28:11.739164Z","iopub.execute_input":"2025-04-06T19:28:11.739553Z","iopub.status.idle":"2025-04-06T19:28:11.773199Z","shell.execute_reply.started":"2025-04-06T19:28:11.739519Z","shell.execute_reply":"2025-04-06T19:28:11.772099Z"}},"outputs":[{"name":"stdout","text":"Successfully added ratio columns and saved to data_with_ratios.csv\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5}]}
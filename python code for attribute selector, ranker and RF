{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313cd03b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-21T17:05:15.138870Z",
     "iopub.status.busy": "2025-05-21T17:05:15.138288Z",
     "iopub.status.idle": "2025-05-21T17:05:16.474233Z",
     "shell.execute_reply": "2025-05-21T17:05:16.472480Z"
    },
    "papermill": {
     "duration": 1.345208,
     "end_time": "2025-05-21T17:05:16.476924",
     "exception": false,
     "start_time": "2025-05-21T17:05:15.131716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/acf-table-x/ACF_table.csv\n",
      "/kaggle/input/new-dataset/classification_FD.csv\n",
      "/kaggle/input/pcom-jcom/combined_output2.csv\n",
      "/kaggle/input/ped-data/data_with_ratios.csv\n",
      "/kaggle/input/acf-fd-table/FD_table.csv\n",
      "/kaggle/input/be-data/classification_FD_nonphysical.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb67d5",
   "metadata": {
    "papermill": {
     "duration": 0.006207,
     "end_time": "2025-05-21T17:05:16.490294",
     "exception": false,
     "start_time": "2025-05-21T17:05:16.484087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Attribute selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209c0c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:26:27.534202Z",
     "iopub.status.busy": "2025-05-21T15:26:27.533819Z",
     "iopub.status.idle": "2025-05-21T15:26:29.060959Z",
     "shell.execute_reply": "2025-05-21T15:26:29.059658Z",
     "shell.execute_reply.started": "2025-05-21T15:26:27.534172Z"
    },
    "papermill": {
     "duration": 0.003717,
     "end_time": "2025-05-21T17:05:16.498777",
     "exception": false,
     "start_time": "2025-05-21T17:05:16.495060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ac6c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T17:05:16.508797Z",
     "iopub.status.busy": "2025-05-21T17:05:16.508009Z",
     "iopub.status.idle": "2025-05-21T17:05:19.498599Z",
     "shell.execute_reply": "2025-05-21T17:05:19.497039Z"
    },
    "papermill": {
     "duration": 2.998158,
     "end_time": "2025-05-21T17:05:19.500952",
     "exception": false,
     "start_time": "2025-05-21T17:05:16.502794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Weka-style): ['pcom_pre4', 'pcom_pre5', 'pcom_ratio5']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data and filter columns starting with 'pcom' or 'Jcom'.\"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    # Keep only relevant columns\n",
    "    selected_columns = [col for col in data.columns]# if col.startswith((\"pcom\", \"Jcom\"))]\n",
    "    X = data[selected_columns]  # Select only matching columns\n",
    "    X = X.drop(['Sub_code','resp'], axis=1)\n",
    "    y = data['resp'].map({'performer': 1, 'nonperformer': 0}).fillna(0).astype(int)\n",
    "    return X, y\n",
    "\n",
    "class WekaStyleSelector:\n",
    "    def __init__(self, k=3, seed=1, num_folds=10):\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "        self.num_folds = num_folds\n",
    "        self.pipeline = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the pipeline (feature selection + classifier).\"\"\"\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=self.k)\n",
    "        clf = RandomForestClassifier(random_state=self.seed)\n",
    "        self.pipeline = Pipeline([('selector', selector), ('clf', clf)])\n",
    "        self.pipeline.fit(X, y)  # Fit on entire data (for inspection)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Return features selected during fitting.\"\"\"\n",
    "        return self.pipeline.named_steps['selector'].transform(X)\n",
    "\n",
    "    def cross_validate(self, X, y):\n",
    "        \"\"\"Perform CV with per-fold feature selection (like Weka's FilteredClassifier).\"\"\"\n",
    "        cv = StratifiedKFold(n_splits=self.num_folds, shuffle=True, random_state=self.seed)\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=self.k)\n",
    "        pipeline = Pipeline([('selector', selector), ('clf', RandomForestClassifier(random_state=self.seed))])\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, n_jobs=-1)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def get_selected_feature_names(self):\n",
    "        \"\"\"Get names of features selected during fitting.\"\"\"\n",
    "        mask = self.pipeline.named_steps['selector'].get_support()\n",
    "        return [self.feature_names[i] for i in np.where(mask)[0]]\n",
    "\n",
    "# Load and shuffle data (Weka does not auto-shuffle; remove if undesired)\n",
    "X, y = load_data('/kaggle/input/ped-data/data_with_ratios.csv')\n",
    "X, y = shuffle(X, y, random_state=1)\n",
    "\n",
    "# Initialize and run selector\n",
    "selector = WekaStyleSelector(k=3, seed=1)\n",
    "selector.fit(X, y)\n",
    "\n",
    "\n",
    "# Get selected features (from full-dataset fit)\n",
    "selected_features = selector.get_selected_feature_names()\n",
    "print(\"Selected Features (Weka-style):\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bbe487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T17:05:19.510943Z",
     "iopub.status.busy": "2025-05-21T17:05:19.510498Z",
     "iopub.status.idle": "2025-05-21T17:05:19.519019Z",
     "shell.execute_reply": "2025-05-21T17:05:19.517778Z"
    },
    "papermill": {
     "duration": 0.015806,
     "end_time": "2025-05-21T17:05:19.520945",
     "exception": false,
     "start_time": "2025-05-21T17:05:19.505139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pcom_pre4', 'pcom_pre5', 'pcom_ratio5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a4776",
   "metadata": {
    "papermill": {
     "duration": 0.003802,
     "end_time": "2025-05-21T17:05:19.529054",
     "exception": false,
     "start_time": "2025-05-21T17:05:19.525252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15429630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T17:05:19.539080Z",
     "iopub.status.busy": "2025-05-21T17:05:19.538640Z",
     "iopub.status.idle": "2025-05-21T17:05:19.647848Z",
     "shell.execute_reply": "2025-05-21T17:05:19.646358Z"
    },
    "papermill": {
     "duration": 0.117162,
     "end_time": "2025-05-21T17:05:19.650303",
     "exception": false,
     "start_time": "2025-05-21T17:05:19.533141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Ranking based on Information Gain:\n",
      "Rank\tFeature\t\tInformation Gain\n",
      "-------------------------------------------\n",
      "1\tpcom_pre5\t\t0.1575\n",
      "2\tpcom_pre4\t\t0.1430\n",
      "3\tpcom_ratio5\t\t0.0791\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "class InfoGainAttributeEval(BaseEstimator):\n",
    "    \"\"\"Information Gain attribute evaluator\"\"\"\n",
    "    \n",
    "    def __init__(self, missing_merge=True, binarize=False):\n",
    "        self.missing_merge = missing_merge\n",
    "        self.binarize = binarize\n",
    "        self.info_gains_ = None\n",
    "        self.valid_features_ = []\n",
    "        self.feature_names_ = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the information gain evaluator\"\"\"\n",
    "        # Add feature names capture\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_ = X.columns.tolist()\n",
    "        else:\n",
    "            self.feature_names_ = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "        X, y = check_X_y(X, y, dtype=None, force_all_finite='allow-nan')\n",
    "        self.valid_features_ = list(range(X.shape[1]))\n",
    "        self._preprocess_data(X, y)\n",
    "        return self\n",
    "        \n",
    "    def _preprocess_data(self, X, y):\n",
    "        \"\"\"Handle discretization/binarization and missing values\"\"\"\n",
    "        self.df = pd.DataFrame(X)\n",
    "        self.classes_ = pd.Series(y).unique()\n",
    "        \n",
    "        # Handle numeric features\n",
    "        for col in self.df.select_dtypes(include='number'):\n",
    "            if self.binarize:\n",
    "                self.df[col] = self._binarize(self.df[col])\n",
    "            else:\n",
    "                self.df[col] = self._discretize(self.df[col])\n",
    "                \n",
    "        # Store processed data and class labels\n",
    "        self.df['__class__'] = y\n",
    "        self._build_contingency_tables()\n",
    "        \n",
    "    def _discretize(self, feature):\n",
    "        \"\"\"Discretize numeric features using KBinsDiscretizer\"\"\"\n",
    "        discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "        return discretizer.fit_transform(feature.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    def _binarize(self, feature):\n",
    "        \"\"\"Binarize numeric features using median threshold\"\"\"\n",
    "        return (feature > feature.median()).astype(int)\n",
    "    \n",
    "    def _build_contingency_tables(self):\n",
    "        \"\"\"Build contingency tables for each feature\"\"\"\n",
    "        self.contingency_tables = {}\n",
    "        \n",
    "        for col in self.df.columns[:-1]:  # Exclude class column\n",
    "            # Create contingency table\n",
    "            cont_table = pd.crosstab(\n",
    "                self.df[col], \n",
    "                self.df['__class__'],\n",
    "                rownames=[col],\n",
    "                colnames=['class'],\n",
    "                dropna=False\n",
    "            )\n",
    "            \n",
    "            # Handle missing values\n",
    "            if self.missing_merge:\n",
    "                cont_table = self._distribute_missing(cont_table)\n",
    "                \n",
    "            self.contingency_tables[col] = cont_table\n",
    "            \n",
    "    def _distribute_missing(self, cont_table):\n",
    "        \"\"\"Distribute missing values proportionally\"\"\"\n",
    "        # Calculate missing proportions\n",
    "        row_missing = cont_table.loc[np.nan] if np.nan in cont_table.index else pd.Series(0, index=cont_table.columns)\n",
    "        col_missing = cont_table.loc[:, np.nan] if np.nan in cont_table.columns else pd.Series(0, index=cont_table.index)\n",
    "        \n",
    "        # Remove missing entries\n",
    "        cont_table = cont_table.dropna(how='any', axis=0)\n",
    "        cont_table = cont_table.dropna(how='any', axis=1)\n",
    "        \n",
    "        # Calculate distribution proportions\n",
    "        row_totals = cont_table.sum(axis=1)\n",
    "        col_totals = cont_table.sum(axis=0)\n",
    "        total = cont_table.sum().sum()\n",
    "        \n",
    "        # Distribute row missing values\n",
    "        for idx, count in row_missing.items():\n",
    "            if count > 0 and total > 0:\n",
    "                proportions = row_totals / total\n",
    "                cont_table.loc[:, idx] += proportions * count\n",
    "                \n",
    "        # Distribute column missing values\n",
    "        for idx, count in col_missing.items():\n",
    "            if count > 0 and total > 0:\n",
    "                proportions = col_totals / total\n",
    "                cont_table.loc[idx, :] += proportions * count\n",
    "                \n",
    "        return cont_table.fillna(0)\n",
    "    \n",
    "    def _calculate_entropy(self, cont_table):\n",
    "        \"\"\"Calculate entropy for a contingency table\"\"\"\n",
    "        class_counts = cont_table.sum(axis=0)\n",
    "        total = class_counts.sum()\n",
    "        class_probs = class_counts / total\n",
    "        return entropy(class_probs, base=2)\n",
    "    \n",
    "    def _calculate_conditional_entropy(self, cont_table):\n",
    "        \"\"\"Calculate conditional entropy for a feature\"\"\"\n",
    "        feature_counts = cont_table.sum(axis=1)\n",
    "        total = feature_counts.sum()\n",
    "        entropies = []\n",
    "        \n",
    "        for _, row in cont_table.iterrows():\n",
    "            row_total = row.sum()\n",
    "            if row_total == 0:\n",
    "                continue\n",
    "            probs = row / row_total\n",
    "            ent = entropy(probs, base=2)\n",
    "            entropies.append((row_total / total) * ent)\n",
    "            \n",
    "        return sum(entropies)\n",
    "    \n",
    "    def evaluate_attribute(self, attribute_idx):\n",
    "        \"\"\"Evaluate information gain for a specific attribute\"\"\"\n",
    "        if self.info_gains_ is None:\n",
    "            self._compute_info_gains()\n",
    "            \n",
    "        if attribute_idx >= len(self.info_gains_):\n",
    "            raise ValueError(f\"Invalid attribute index: {attribute_idx}\")\n",
    "            \n",
    "        return self.info_gains_[attribute_idx]\n",
    "    \n",
    "    def _compute_info_gains(self):\n",
    "        \"\"\"Compute information gains for all attributes\"\"\"\n",
    "        self.info_gains_ = []\n",
    "        \n",
    "        # Calculate overall entropy\n",
    "        class_counts = self.df['__class__'].value_counts()\n",
    "        total = class_counts.sum()\n",
    "        class_probs = class_counts / total\n",
    "        H_class = entropy(class_probs, base=2)\n",
    "        \n",
    "        for col in self.df.columns[:-1]:\n",
    "            cont_table = self.contingency_tables[col]\n",
    "            H_conditional = self._calculate_conditional_entropy(cont_table)\n",
    "            self.info_gains_.append(H_class - H_conditional)\n",
    "    \n",
    "    def get_ranked_features(self):\n",
    "        \"\"\"Return features ranked by information gain\"\"\"\n",
    "        if self.info_gains_ is None:\n",
    "            self._compute_info_gains()\n",
    "            \n",
    "        # Create list of (index, gain) pairs\n",
    "        features = list(range(len(self.info_gains_)))\n",
    "        ranked = sorted(zip(features, self.info_gains_), \n",
    "                        key=lambda x: x[1], reverse=True)\n",
    "        return ranked\n",
    "\n",
    "    def print_ranking(self):\n",
    "        \"\"\"Print feature ranking with information gain scores\"\"\"\n",
    "        ranked = self.get_ranked_features()\n",
    "        print(\"\\nFeature Ranking based on Information Gain:\")\n",
    "        print(\"Rank\\tFeature\\t\\tInformation Gain\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        for i, (feat_idx, gain) in enumerate(ranked):\n",
    "            # Use actual feature names\n",
    "            feat_name = self.feature_names_[feat_idx]\n",
    "            print(f\"{i+1}\\t{feat_name}\\t\\t{gain:.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Initialize and fit evaluator\n",
    "ig = InfoGainAttributeEval()\n",
    "X=X[selected_features]\n",
    "ig.fit(X, y)\n",
    "    \n",
    "ranked = ig.print_ranking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714b652",
   "metadata": {
    "papermill": {
     "duration": 0.004506,
     "end_time": "2025-05-21T17:05:19.659958",
     "exception": false,
     "start_time": "2025-05-21T17:05:19.655452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bd6c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T17:05:19.671201Z",
     "iopub.status.busy": "2025-05-21T17:05:19.670761Z",
     "iopub.status.idle": "2025-05-21T17:05:24.024590Z",
     "shell.execute_reply": "2025-05-21T17:05:24.022774Z"
    },
    "papermill": {
     "duration": 4.362903,
     "end_time": "2025-05-21T17:05:24.027403",
     "exception": false,
     "start_time": "2025-05-21T17:05:19.664500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Classified Instances: 75.0 (75.00%)\n",
      "Incorrectly Classified Instances: 25.0 (25.00%)\n",
      "Kappa Statistic: 0.4839\n",
      "Matthews Correlation Coefficient: 0.4890\n",
      "ROC AUC: 0.7689\n",
      "PRC AUC: 0.7312\n",
      "Precision (Performer): 0.7568\n",
      "Recall (Performer): 0.6364\n",
      "F1-Score (Performer): 0.6914\n",
      "Weighted Avg F1-Score: 0.7466\n",
      "Mean Absolute Error: 0.2500\n",
      "Root Mean Squared Error: 0.5000\n",
      "Relative Absolute Error: 0.4464\n",
      "Root Relative Squared Error: 0.6682\n",
      "\n",
      "Confusion Matrix:\n",
      "                       Nonperformer (Predicted)  Performer (Predicted)\n",
      "Nonperformer (Actual)                        28                     16\n",
      "Performer (Actual)                            9                     47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                             cohen_kappa_score, roc_auc_score, precision_recall_curve, auc,\n",
    "                             matthews_corrcoef)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "class WekaStyleRandomForest:\n",
    "    def __init__(self, num_trees=100, max_depth=None, seed=1):\n",
    "        \"\"\"\n",
    "        Mirrors Weka's RandomForest defaults:\n",
    "        - numTrees = 100\n",
    "        - maxDepth = unlimited (None)\n",
    "        - use entropy for splitting (criterion='entropy')\n",
    "        - compute out-of-bag (OOB) error (oob_score=True)\n",
    "        - feature selection: log2(num_features) + 1 (max_features='log2')\n",
    "        \"\"\"\n",
    "        self.classifier = RandomForestClassifier(\n",
    "            n_estimators=num_trees,\n",
    "            criterion='entropy',  # Weka uses entropy, not Gini\n",
    "            max_depth=max_depth,\n",
    "            max_features='log2',   # Weka's default feature selection\n",
    "            oob_score=True,        # Enable OOB error (like Weka)\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    def cross_validate(self, X, y, num_folds=10):\n",
    "        \"\"\"Return both predicted labels and probabilities.\"\"\"\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=False)\n",
    "        y_proba = cross_val_predict(\n",
    "            self.classifier, X, y, cv=skf, method='predict_proba'\n",
    "        )[:, 1]  # Probabilities of class 1\n",
    "        y_pred = np.round(y_proba).astype(int)  # Convert to class labels\n",
    "        return y_pred, y_proba  # Return both\n",
    "\n",
    "    def get_weka_metrics(self, y_true, y_pred, y_proba):\n",
    "        \"\"\"Use probabilities to compute ROC/PRC AUC.\"\"\"\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        report = classification_report(y_true, y_pred, target_names=['Performer', 'NonPerpormer'], output_dict=True)\n",
    "        \n",
    "        # Compute AUC using probabilities\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "        prc_auc = auc(recall, precision)\n",
    "        \n",
    " # Regression-style metrics\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        relative_absolute_error = mae / np.mean(np.abs(y_true))\n",
    "        root_relative_squared_error = rmse / np.sqrt(np.mean(y_true**2))\n",
    "        \n",
    "        return {\n",
    "            \"confusion_matrix\": [[tn, fp], [fn, tp]],\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"prc_auc\": prc_auc,\n",
    "            \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "            \"precision\": report['Performer']['precision'],\n",
    "            \"recall\": report['Performer']['recall'],\n",
    "            \"f1\": report['Performer']['f1-score'],\n",
    "            \"weighted_avg_f1\": report['weighted avg']['f1-score'],\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse,\n",
    "            \"relative_absolute_error\": relative_absolute_error,\n",
    "            \"root_relative_squared_error\": root_relative_squared_error\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Weka-style classifier\n",
    "rf = WekaStyleRandomForest(num_trees=100, max_depth=None, seed=1)\n",
    "\n",
    "# Cross-validate (no shuffling, 10 folds)\n",
    "y_pred, y_proba = rf.cross_validate(X, y, num_folds=10)\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "metrics = rf.get_weka_metrics(y, y_pred, y_proba)  # Add y_proba if using predict_proba\n",
    "\n",
    "\n",
    "# Print results (Weka-like format)\n",
    "print(f\"Correctly Classified Instances: {metrics['accuracy'] * len(y)} ({metrics['accuracy'] * 100:.2f}%)\")\n",
    "print(f\"Incorrectly Classified Instances: {(1 - metrics['accuracy']) * len(y)} ({(1 - metrics['accuracy']) * 100:.2f}%)\")\n",
    "print(f\"Kappa Statistic: {metrics['kappa']:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {metrics['mcc']:.4f}\")\n",
    "print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "print(f\"PRC AUC: {metrics['prc_auc']:.4f}\")\n",
    "print(f\"Precision (Performer): {metrics['precision']:.4f}\")\n",
    "print(f\"Recall (Performer): {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score (Performer): {metrics['f1']:.4f}\")\n",
    "print(f\"Weighted Avg F1-Score: {metrics['weighted_avg_f1']:.4f}\")\n",
    "print(f\"Mean Absolute Error: {metrics['mae']:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {metrics['rmse']:.4f}\")\n",
    "print(f\"Relative Absolute Error: {metrics['relative_absolute_error']:.4f}\")\n",
    "print(f\"Root Relative Squared Error: {metrics['root_relative_squared_error']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(metrics['confusion_matrix'], \n",
    "                   index=['Nonperformer (Actual)', 'Performer (Actual)'], \n",
    "                   columns=['Nonperformer (Predicted)', 'Performer (Predicted)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbc74f",
   "metadata": {
    "papermill": {
     "duration": 0.0042,
     "end_time": "2025-05-21T17:05:24.036412",
     "exception": false,
     "start_time": "2025-05-21T17:05:24.032212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Compute Ratios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088d7e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T17:05:24.047688Z",
     "iopub.status.busy": "2025-05-21T17:05:24.047204Z",
     "iopub.status.idle": "2025-05-21T17:05:24.059859Z",
     "shell.execute_reply": "2025-05-21T17:05:24.058627Z"
    },
    "papermill": {
     "duration": 0.021142,
     "end_time": "2025-05-21T17:05:24.061926",
     "exception": false,
     "start_time": "2025-05-21T17:05:24.040784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: 'stacked_hemispheres_data.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_ratios(input_file, output_file):\n",
    "    try:\n",
    "        # Read the stacked CSV file\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Add ratio columns for pcom values\n",
    "        # pcom_ratio1 = pcom_pre1/pcom_pre1\n",
    "        df['pcom_ratio1'] = df['pcom_pre1'] / df['pcom_pre1']  # This will be 1.0 for all non-NaN values\n",
    "        \n",
    "        # pcom_ratio2 through pcom_ratio5\n",
    "        for i in range(2, 6):\n",
    "            df[f'pcom_ratio{i}'] = df[f'pcom_pre{i}'] / df['pcom_pre1']\n",
    "        \n",
    "        # pcom_ratio6 through pcom_ratio10\n",
    "        for i in range(1, 6):\n",
    "            ratio_index = i + 5\n",
    "            df[f'pcom_ratio{ratio_index}'] = df[f'pcom_post{i}'] / df['pcom_pre1']\n",
    "        \n",
    "        # Add ratio columns for Jcom values - same pattern as pcom\n",
    "        # Jcom_ratio1 = Jcom_pre1/Jcom_pre1\n",
    "        df['Jcom_ratio1'] = df['Jcom_pre1'] / df['Jcom_pre1']  # This will be 1.0 for all non-NaN values\n",
    "        \n",
    "        # Jcom_ratio2 through Jcom_ratio5\n",
    "        for i in range(2, 6):\n",
    "            df[f'Jcom_ratio{i}'] = df[f'Jcom_pre{i}'] / df['Jcom_pre1']\n",
    "        \n",
    "        # Jcom_ratio6 through Jcom_ratio10\n",
    "        for i in range(1, 6):\n",
    "            ratio_index = i + 5\n",
    "            df[f'Jcom_ratio{ratio_index}'] = df[f'Jcom_post{i}'] / df['Jcom_pre1']\n",
    "        \n",
    "        # Save to a new CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Successfully added ratio columns and saved to {output_file}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "input_file = \"stacked_hemispheres_data.csv\"  # Output from previous script\n",
    "output_file = \"data_with_ratios.csv\"  \n",
    "calculate_ratios(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6791286,
     "sourceId": 10923340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6791291,
     "sourceId": 10923348,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6791296,
     "sourceId": 10923354,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7067380,
     "sourceId": 11301114,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7067598,
     "sourceId": 11301391,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7212032,
     "sourceId": 11503208,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.681705,
   "end_time": "2025-05-21T17:05:25.192840",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T17:05:10.511135",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

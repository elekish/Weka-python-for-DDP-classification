{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9855ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-11T16:48:55.464337Z",
     "iopub.status.busy": "2025-03-11T16:48:55.463921Z",
     "iopub.status.idle": "2025-03-11T16:48:56.616737Z",
     "shell.execute_reply": "2025-03-11T16:48:56.615324Z"
    },
    "papermill": {
     "duration": 1.15987,
     "end_time": "2025-03-11T16:48:56.618785",
     "exception": false,
     "start_time": "2025-03-11T16:48:55.458915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/acf-table-x/ACF_table.csv\n",
      "/kaggle/input/acf-fd-table/FD_table.csv\n",
      "/kaggle/input/pcom-jcom/combined_output2.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd463d31",
   "metadata": {
    "papermill": {
     "duration": 0.00285,
     "end_time": "2025-03-11T16:48:56.625185",
     "exception": false,
     "start_time": "2025-03-11T16:48:56.622335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Attribute selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e298336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:48:56.633008Z",
     "iopub.status.busy": "2025-03-11T16:48:56.632529Z",
     "iopub.status.idle": "2025-03-11T16:49:00.491579Z",
     "shell.execute_reply": "2025-03-11T16:49:00.489940Z"
    },
    "papermill": {
     "duration": 3.865317,
     "end_time": "2025-03-11T16:49:00.493641",
     "exception": false,
     "start_time": "2025-03-11T16:48:56.628324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Names: ['jcom_pre2', 'jcom_pre4', 'jcom_pre5', 'jcom_post2', 'jcom_post3', 'J2max_pre2', 'J2max_pre5', 'J2max_post2', 'J2max_post4', 'J2max_post5', 'pcom_pre1', 'pcom_pre2', 'pcom_pre3', 'pcom_pre4', 'pcom_pre5', 'pcom_post1', 'pcom_post2', 'pcom_post3', 'pcom_post4', 'pcom_post5']\n",
      "Cross-Validation Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load dataset from a file and keep only columns that start with 'pcom', 'Jcom', or 'J2max'.\n",
    "    \n",
    "    :param filepath: Path to the dataset file (CSV).\n",
    "    :return: Tuple (X, y) where X are the filtered features and y is the target.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    selected_columns = [col for col in data.columns if col.startswith((\"pcom\", \"jcom\", \"J2max\"))]\n",
    "    X = data[selected_columns]  # Select only matching columns\n",
    "    y = data.iloc[:, -1]        # Last column as target\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "class AttributeSelection:\n",
    "    def __init__(self, k=10, method=\"f_classif\", seed=1, num_folds=10):\n",
    "        \"\"\"\n",
    "        Initializes the attribute selection model.\n",
    "        \n",
    "        :param k: Number of top features to select.\n",
    "        :param method: Scoring method ('f_classif' or 'mutual_info').\n",
    "        :param seed: Random seed for reproducibility.\n",
    "        :param num_folds: Number of folds for cross-validation.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.method = method\n",
    "        self.seed = seed\n",
    "        self.num_folds = num_folds\n",
    "        self.selected_features = None\n",
    "        self.selector = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Performs feature selection on the dataset.\n",
    "        \n",
    "        :param X: Feature matrix (numpy array or pandas DataFrame).\n",
    "        :param y: Target labels.\n",
    "        \"\"\"\n",
    "        if self.method == \"f_classif\":\n",
    "            score_func = f_classif\n",
    "        elif self.method == \"mutual_info\":\n",
    "            score_func = mutual_info_classif\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'f_classif' or 'mutual_info'.\")\n",
    "\n",
    "        self.selector = SelectKBest(score_func=score_func, k=self.k)\n",
    "        self.selector.fit(X, y)\n",
    "        self.selected_features = self.selector.get_support(indices=True)\n",
    "        self.feature_names = X.columns  # Store feature names\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the dataset by selecting important features.\n",
    "        \n",
    "        :param X: Feature matrix.\n",
    "        :return: Transformed dataset with selected features.\n",
    "        \"\"\"\n",
    "        if self.selector is None:\n",
    "            raise ValueError(\"Feature selection has not been performed yet!\")\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def cross_validate(self, X, y, model):\n",
    "        \"\"\"\n",
    "        Performs cross-validation to evaluate feature selection effectiveness.\n",
    "        \n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target labels.\n",
    "        :param model: Machine learning model for evaluation.\n",
    "        :return: Mean accuracy score from cross-validation.\n",
    "        \"\"\"\n",
    "        X_selected = self.transform(X)\n",
    "        scores = cross_val_score(model, X_selected, y, cv=self.num_folds)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def get_selected_features(self):\n",
    "        \"\"\"\n",
    "        Returns the selected feature indices.\n",
    "        \n",
    "        :return: List of selected feature indices.\n",
    "        \"\"\"\n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"Feature selection has not been performed yet!\")\n",
    "        return self.selected_features\n",
    "\n",
    "    def get_selected_feature_names(self):\n",
    "        \"\"\"\n",
    "        Get the names of selected features.\n",
    "        \n",
    "        :return: List of selected feature names.\n",
    "        \"\"\"\n",
    "        if self.selector is None or self.feature_names is None:\n",
    "            raise ValueError(\"Feature selection has not been performed yet!\")\n",
    "        return self.feature_names[self.selected_features].tolist()\n",
    "\n",
    "\n",
    "# Load dataset and filter only required columns\n",
    "X, y = load_data('/kaggle/input/pcom-jcom/combined_output2.csv')\n",
    "\n",
    "# Shuffle dataset\n",
    "X, y = shuffle(X, y, random_state=1)\n",
    "\n",
    "# Perform attribute selection\n",
    "selector = AttributeSelection(k=20, method=\"f_classif\")\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Transform dataset\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "# Print selected feature names\n",
    "selected_features=selector.get_selected_feature_names()\n",
    "print(\"Selected Feature Names:\", selected_features)\n",
    "\n",
    "# Evaluate with a classifier\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "score = selector.cross_validate(X, y, model)\n",
    "print(f\"Cross-Validation Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2391ee84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:49:00.502085Z",
     "iopub.status.busy": "2025-03-11T16:49:00.501577Z",
     "iopub.status.idle": "2025-03-11T16:49:00.509439Z",
     "shell.execute_reply": "2025-03-11T16:49:00.508234Z"
    },
    "papermill": {
     "duration": 0.014401,
     "end_time": "2025-03-11T16:49:00.511657",
     "exception": false,
     "start_time": "2025-03-11T16:49:00.497256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jcom_pre2',\n",
       " 'jcom_pre4',\n",
       " 'jcom_pre5',\n",
       " 'jcom_post2',\n",
       " 'jcom_post3',\n",
       " 'J2max_pre2',\n",
       " 'J2max_pre5',\n",
       " 'J2max_post2',\n",
       " 'J2max_post4',\n",
       " 'J2max_post5',\n",
       " 'pcom_pre1',\n",
       " 'pcom_pre2',\n",
       " 'pcom_pre3',\n",
       " 'pcom_pre4',\n",
       " 'pcom_pre5',\n",
       " 'pcom_post1',\n",
       " 'pcom_post2',\n",
       " 'pcom_post3',\n",
       " 'pcom_post4',\n",
       " 'pcom_post5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2bf82",
   "metadata": {
    "papermill": {
     "duration": 0.003127,
     "end_time": "2025-03-11T16:49:00.518473",
     "exception": false,
     "start_time": "2025-03-11T16:49:00.515346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610afcb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:49:00.526632Z",
     "iopub.status.busy": "2025-03-11T16:49:00.526277Z",
     "iopub.status.idle": "2025-03-11T16:49:00.856843Z",
     "shell.execute_reply": "2025-03-11T16:49:00.855466Z"
    },
    "papermill": {
     "duration": 0.336976,
     "end_time": "2025-03-11T16:49:00.858765",
     "exception": false,
     "start_time": "2025-03-11T16:49:00.521789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Ranking based on Information Gain:\n",
      "Rank\tFeature\t\tInformation Gain\n",
      "-------------------------------------------\n",
      "1\tpcom_pre1\t\t1.0000\n",
      "2\tpcom_pre2\t\t1.0000\n",
      "3\tpcom_pre3\t\t1.0000\n",
      "4\tpcom_pre4\t\t1.0000\n",
      "5\tpcom_pre5\t\t1.0000\n",
      "6\tpcom_post1\t\t1.0000\n",
      "7\tpcom_post2\t\t1.0000\n",
      "8\tpcom_post3\t\t1.0000\n",
      "9\tpcom_post4\t\t1.0000\n",
      "10\tpcom_post5\t\t1.0000\n",
      "11\tJ2max_pre5\t\t0.5665\n",
      "12\tJ2max_post5\t\t0.5222\n",
      "13\tJ2max_post4\t\t0.4861\n",
      "14\tJ2max_pre2\t\t0.3711\n",
      "15\tJ2max_post2\t\t0.3228\n",
      "16\tjcom_pre2\t\t0.2243\n",
      "17\tjcom_pre5\t\t0.1307\n",
      "18\tjcom_post2\t\t0.1201\n",
      "19\tjcom_pre4\t\t0.0860\n",
      "20\tjcom_post3\t\t0.0634\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "class InfoGainAttributeEval(BaseEstimator):\n",
    "    \"\"\"Information Gain attribute evaluator\"\"\"\n",
    "    \n",
    "    def __init__(self, missing_merge=True, binarize=False):\n",
    "        self.missing_merge = missing_merge\n",
    "        self.binarize = binarize\n",
    "        self.info_gains_ = None\n",
    "        self.valid_features_ = []\n",
    "        self.feature_names_ = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the information gain evaluator\"\"\"\n",
    "        # Add feature names capture\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_ = X.columns.tolist()\n",
    "        else:\n",
    "            self.feature_names_ = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "        X, y = check_X_y(X, y, dtype=None, force_all_finite='allow-nan')\n",
    "        self.valid_features_ = list(range(X.shape[1]))\n",
    "        self._preprocess_data(X, y)\n",
    "        return self\n",
    "        \n",
    "    def _preprocess_data(self, X, y):\n",
    "        \"\"\"Handle discretization/binarization and missing values\"\"\"\n",
    "        self.df = pd.DataFrame(X)\n",
    "        self.classes_ = pd.Series(y).unique()\n",
    "        \n",
    "        # Handle numeric features\n",
    "        for col in self.df.select_dtypes(include='number'):\n",
    "            if self.binarize:\n",
    "                self.df[col] = self._binarize(self.df[col])\n",
    "            else:\n",
    "                self.df[col] = self._discretize(self.df[col])\n",
    "                \n",
    "        # Store processed data and class labels\n",
    "        self.df['__class__'] = y\n",
    "        self._build_contingency_tables()\n",
    "        \n",
    "    def _discretize(self, feature):\n",
    "        \"\"\"Discretize numeric features using KBinsDiscretizer\"\"\"\n",
    "        discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "        return discretizer.fit_transform(feature.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    def _binarize(self, feature):\n",
    "        \"\"\"Binarize numeric features using median threshold\"\"\"\n",
    "        return (feature > feature.median()).astype(int)\n",
    "    \n",
    "    def _build_contingency_tables(self):\n",
    "        \"\"\"Build contingency tables for each feature\"\"\"\n",
    "        self.contingency_tables = {}\n",
    "        \n",
    "        for col in self.df.columns[:-1]:  # Exclude class column\n",
    "            # Create contingency table\n",
    "            cont_table = pd.crosstab(\n",
    "                self.df[col], \n",
    "                self.df['__class__'],\n",
    "                rownames=[col],\n",
    "                colnames=['class'],\n",
    "                dropna=False\n",
    "            )\n",
    "            \n",
    "            # Handle missing values\n",
    "            if self.missing_merge:\n",
    "                cont_table = self._distribute_missing(cont_table)\n",
    "                \n",
    "            self.contingency_tables[col] = cont_table\n",
    "            \n",
    "    def _distribute_missing(self, cont_table):\n",
    "        \"\"\"Distribute missing values proportionally\"\"\"\n",
    "        # Calculate missing proportions\n",
    "        row_missing = cont_table.loc[np.nan] if np.nan in cont_table.index else pd.Series(0, index=cont_table.columns)\n",
    "        col_missing = cont_table.loc[:, np.nan] if np.nan in cont_table.columns else pd.Series(0, index=cont_table.index)\n",
    "        \n",
    "        # Remove missing entries\n",
    "        cont_table = cont_table.dropna(how='any', axis=0)\n",
    "        cont_table = cont_table.dropna(how='any', axis=1)\n",
    "        \n",
    "        # Calculate distribution proportions\n",
    "        row_totals = cont_table.sum(axis=1)\n",
    "        col_totals = cont_table.sum(axis=0)\n",
    "        total = cont_table.sum().sum()\n",
    "        \n",
    "        # Distribute row missing values\n",
    "        for idx, count in row_missing.items():\n",
    "            if count > 0 and total > 0:\n",
    "                proportions = row_totals / total\n",
    "                cont_table.loc[:, idx] += proportions * count\n",
    "                \n",
    "        # Distribute column missing values\n",
    "        for idx, count in col_missing.items():\n",
    "            if count > 0 and total > 0:\n",
    "                proportions = col_totals / total\n",
    "                cont_table.loc[idx, :] += proportions * count\n",
    "                \n",
    "        return cont_table.fillna(0)\n",
    "    \n",
    "    def _calculate_entropy(self, cont_table):\n",
    "        \"\"\"Calculate entropy for a contingency table\"\"\"\n",
    "        class_counts = cont_table.sum(axis=0)\n",
    "        total = class_counts.sum()\n",
    "        class_probs = class_counts / total\n",
    "        return entropy(class_probs, base=2)\n",
    "    \n",
    "    def _calculate_conditional_entropy(self, cont_table):\n",
    "        \"\"\"Calculate conditional entropy for a feature\"\"\"\n",
    "        feature_counts = cont_table.sum(axis=1)\n",
    "        total = feature_counts.sum()\n",
    "        entropies = []\n",
    "        \n",
    "        for _, row in cont_table.iterrows():\n",
    "            row_total = row.sum()\n",
    "            if row_total == 0:\n",
    "                continue\n",
    "            probs = row / row_total\n",
    "            ent = entropy(probs, base=2)\n",
    "            entropies.append((row_total / total) * ent)\n",
    "            \n",
    "        return sum(entropies)\n",
    "    \n",
    "    def evaluate_attribute(self, attribute_idx):\n",
    "        \"\"\"Evaluate information gain for a specific attribute\"\"\"\n",
    "        if self.info_gains_ is None:\n",
    "            self._compute_info_gains()\n",
    "            \n",
    "        if attribute_idx >= len(self.info_gains_):\n",
    "            raise ValueError(f\"Invalid attribute index: {attribute_idx}\")\n",
    "            \n",
    "        return self.info_gains_[attribute_idx]\n",
    "    \n",
    "    def _compute_info_gains(self):\n",
    "        \"\"\"Compute information gains for all attributes\"\"\"\n",
    "        self.info_gains_ = []\n",
    "        \n",
    "        # Calculate overall entropy\n",
    "        class_counts = self.df['__class__'].value_counts()\n",
    "        total = class_counts.sum()\n",
    "        class_probs = class_counts / total\n",
    "        H_class = entropy(class_probs, base=2)\n",
    "        \n",
    "        for col in self.df.columns[:-1]:\n",
    "            cont_table = self.contingency_tables[col]\n",
    "            H_conditional = self._calculate_conditional_entropy(cont_table)\n",
    "            self.info_gains_.append(H_class - H_conditional)\n",
    "    \n",
    "    def get_ranked_features(self):\n",
    "        \"\"\"Return features ranked by information gain\"\"\"\n",
    "        if self.info_gains_ is None:\n",
    "            self._compute_info_gains()\n",
    "            \n",
    "        # Create list of (index, gain) pairs\n",
    "        features = list(range(len(self.info_gains_)))\n",
    "        ranked = sorted(zip(features, self.info_gains_), \n",
    "                        key=lambda x: x[1], reverse=True)\n",
    "        return ranked\n",
    "\n",
    "    def print_ranking(self):\n",
    "        \"\"\"Print feature ranking with information gain scores\"\"\"\n",
    "        ranked = self.get_ranked_features()\n",
    "        print(\"\\nFeature Ranking based on Information Gain:\")\n",
    "        print(\"Rank\\tFeature\\t\\tInformation Gain\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        for i, (feat_idx, gain) in enumerate(ranked):\n",
    "            # Use actual feature names\n",
    "            feat_name = self.feature_names_[feat_idx]\n",
    "            print(f\"{i+1}\\t{feat_name}\\t\\t{gain:.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Initialize and fit evaluator\n",
    "ig = InfoGainAttributeEval()\n",
    "X=X[selected_features]\n",
    "ig.fit(X, y)\n",
    "    \n",
    "ranked = ig.print_ranking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bce14",
   "metadata": {
    "papermill": {
     "duration": 0.003167,
     "end_time": "2025-03-11T16:49:00.866337",
     "exception": false,
     "start_time": "2025-03-11T16:49:00.863170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d79fd",
   "metadata": {
    "papermill": {
     "duration": 0.003107,
     "end_time": "2025-03-11T16:49:00.872926",
     "exception": false,
     "start_time": "2025-03-11T16:49:00.869819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14f1caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T16:49:00.881234Z",
     "iopub.status.busy": "2025-03-11T16:49:00.880856Z",
     "iopub.status.idle": "2025-03-11T16:49:02.533557Z",
     "shell.execute_reply": "2025-03-11T16:49:02.531909Z"
    },
    "papermill": {
     "duration": 1.65932,
     "end_time": "2025-03-11T16:49:02.535637",
     "exception": false,
     "start_time": "2025-03-11T16:49:00.876317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jcom_pre2', 'jcom_pre4', 'jcom_pre5', 'jcom_post2', 'jcom_post3', 'J2max_pre2', 'J2max_pre5', 'J2max_post2', 'J2max_post4', 'J2max_post5']\n",
      "Cross-Validation Accuracy: 94.64%\n",
      "Confusion Matrix:\n",
      "[[28  0]\n",
      " [ 3 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Nonperformer       0.90      1.00      0.95        28\n",
      "   Performer       1.00      0.89      0.94        28\n",
      "\n",
      "    accuracy                           0.95        56\n",
      "   macro avg       0.95      0.95      0.95        56\n",
      "weighted avg       0.95      0.95      0.95        56\n",
      "\n",
      "Correctly Classified Instances: 53.0 (94.64%)\n",
      "Incorrectly Classified Instances: 3.0000000000000018 (5.36%)\n",
      "Kappa Statistic: 0.8929\n",
      "Mean Absolute Error: 0.0536\n",
      "Root Mean Squared Error: 0.2315\n",
      "Relative Absolute Error: 0.1071\n",
      "Root Relative Squared Error: 0.3273\n",
      "Total Number of Instances: 56\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, mean_absolute_error,\n",
    "                             mean_squared_error, cohen_kappa_score, roc_auc_score, precision_recall_curve, matthews_corrcoef)\n",
    "\n",
    "class RandomForestKaggle:\n",
    "    def __init__(self, num_trees=100, max_depth=5, random_seed=42, num_execution_slots=-1):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.random_seed = random_seed\n",
    "        self.num_execution_slots = num_execution_slots\n",
    "        self.classifier = RandomForestClassifier(\n",
    "            n_estimators=self.num_trees,\n",
    "            max_depth=self.max_depth,\n",
    "            random_state=self.random_seed,\n",
    "            n_jobs=self.num_execution_slots\n",
    "        )\n",
    "\n",
    "    def cross_validate(self, X, y, num_folds=5):\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=self.random_seed)\n",
    "        y_pred = cross_val_predict(self.classifier, X, y, cv=skf)\n",
    "        return y_pred\n",
    "\n",
    "    def get_feature_importance(self, feature_names):\n",
    "        return dict(zip(feature_names, self.classifier.feature_importances_))\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/kaggle/input/pcom-jcom/combined_output2.csv\")\n",
    "non_pcom_features = [feature for feature in selected_features \n",
    "                    if not str(feature).lower().startswith(\"pcom\")]\n",
    "print(non_pcom_features)\n",
    "#selected_features = ['pcom_post4', 'pcom_post5', 'pcom_post3', 'pcom_post1', 'pcom_pre3', 'pcom_pre2', 'pcom_pre4', 'pcom_pre5', 'pcom_pre1', 'pcom_post2', 'J2max_pre5', 'J2max_post5']\n",
    "class_column = 'resp'\n",
    "\n",
    "df[class_column] = df[class_column].map({'performer': 1, 'nonperformer': 0}).fillna(0).astype(int)\n",
    "X = df[non_pcom_features].values\n",
    "y = df[class_column].values\n",
    "\n",
    "# Initialize classifier\n",
    "rf = RandomForestKaggle()\n",
    "num_folds = 5\n",
    "\n",
    "# Perform cross-validation\n",
    "y_pred = rf.cross_validate(X, y, num_folds)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Cross-Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y, y_pred, target_names=['Nonperformer', 'Performer'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Additional metrics\n",
    "kappa = cohen_kappa_score(y, y_pred)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "relative_absolute_error = mae / np.mean(np.abs(y))\n",
    "root_relative_squared_error = rmse / np.sqrt(np.mean(y**2))\n",
    "\n",
    "print(f\"Correctly Classified Instances: {accuracy * len(y)} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"Incorrectly Classified Instances: {(1 - accuracy) * len(y)} ({(1 - accuracy) * 100:.2f}%)\")\n",
    "print(f\"Kappa Statistic: {kappa:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"Relative Absolute Error: {relative_absolute_error:.4f}\")\n",
    "print(f\"Root Relative Squared Error: {root_relative_squared_error:.4f}\")\n",
    "print(f\"Total Number of Instances: {len(y)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6791286,
     "sourceId": 10923340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6791291,
     "sourceId": 10923348,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6791296,
     "sourceId": 10923354,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.950085,
   "end_time": "2025-03-11T16:49:03.261007",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T16:48:52.310922",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
